\chapter{Theory}
\section{Binary Disassembly}
An assembly language is a programming language that is very close to the actual machine code that is interpreted by a processor. Instead of ones and zeroes however, an assembly language uses mnemonics, or abbreviations, for the instructions available for the computer architecture.


An important aspect of the assembly language in the context of disassembly is that there is a one-to-one mapping between assembly language instructions and instructions in machine code. This means there can be no ambiguities when translating instructions from the assembly language to machine code or from machine code back to the assembly language, although the latter requires an
understanding of the control flow structure of the binary program.

There are a few different dialects of the x86 assembly language, and most notable is the Intel syntax and the AT\&T syntax. The most important differences between these two dialects is (a) the parameter order where Intel places destination before source and AT\&T places source before the destination, (b) that mnemonics in the AT\&T syntax are suffixed with a letter to indicate the
size of the operation whereas this information is derived from the register that is used in the Intel syntax and (c) the general syntax for effective addresses.


Disassembly is the process of transforming a set of machine code instructions, a binary file, into a set of assembly instructions. It is the exact opposite of the assembler process which is usually the last step when compiling a program from source code [15, 16]. Disassembly can either be performed as a static process, where the set of machine code instructions that are being disassembled are never executed, or as a dynamic process, where the set of assembly instructions
that constitutes the machine code is extracted during execution [16]. Static disassembly has the advantage of processing an entire set of machine code instructions at once while dynamic disassembly can only process the subset of machine code instructions that are actually executed. Dynamic disassembly on the other hand has the advantage of extracting a sequence of instructions that is guaranteed to be correct for the execution during which it was extracted.

Decompilation, or reverse compilation, is the process of transforming a set of machine code instructions, or a set of assembly instructions, into a representation in a high-level language [17]. The purpose is to reconstruct the source code of the binary program in a high-level language that can easily be read and understood by a human in order to audit the code or make changes to the functions of the program. Decompilation is a process that is usually performed after disassembly and applies additional transformations to the assembly instructions produced by the disassembly process, thus it should not be seen as a conflicting process to disassembly.

In order to decompile a program correctly, the program’s control flow paths needs to be known, or discovered, during the process. Dynamically, this information can be obtained for the subset of instructions that is actually executed while the program is being monitored. Statically the entire program will be processed, but there will be ambiguities in the result since not all information
regarding the control flow can be known in a static context.

The reason for these ambiguities that arises when statically reconstructing the control flow of a binary files stems from the fact that there is more information present in the original source code. This additional information which primarily concerns control flow is removed when the program is compiled into an executable binary since the information is not needed in order for the program to execute correctly. Fully structured code, where every control-structure has a single point of entry and a single point of exit, should have considerably less ambiguities. This is because once a point of entry is found, the point of exit can be known immediately.

The modern computer programs are developed in programming languages that
are a human readable form [2], [3], [4], [5]. The source code written by software
developers is compiled into a binary format. In software development, there
are two classes of binaries: 

Machine code - is not directly understandable by software developer,
but it is directly executed by the machine; it is generated by compiler depending on the hardware
characteristics; 

Intermediate code - like machine code, is not directly understandable by
software developer and is not directly executed by the machine; the executable code is obtained after an interpreting process performed by a specialized component called virtual
machine; the most known and used virtual machines are Java Virtual Machine and Common Language
Runtime (CLR) [10], [11].

The computer programs delivered in the machine code format are more difficult to be maintained because of the difficulty to understand the executable format. To implement the maintainance activities, the software developer need the source code and documentation. Another way to obtain 
the understandable form of the machine code is to convert it into assembly language.

The disassembly is the process which converts the machine code into equivalent format in assembly language. During this process the assembly instruction set mnemonics are translated into assembly
instructions that can be easily read by software developers.

The practical and positive issues of the disassembly process and its results are :

Improvement of the portability for computer programs delivered in machine code format; unlike machine code, the intermediate code is portable due to its interpreting by a virtual machine which must be mandatorily installed on the host machine;

The software developers determine the logical flows of the disassembled software application; the algorithms and other programming entities are extracted from the software application and used in other versions or programs;

Security issues are identified and can be patched without access to the original source code;

The old version of a computer program is completed with new functionalities and interfaces.

The effects of the disassembly process implementation are quantified in terms of time and costs during the running of the computer program.

The disassembly process is one of the three main classes of techniques for reverse engineering of software [11]. Reverse engineering of software is the process for discovery the technological
principles of a product or system based of analysis of its structure, function and operation [17].

As negative issue, the disassembly process can be carried out by malicious software developers to discover the vulnerabilities and holes of the computer programs to hack them. Also, the discovered logical flows and algorithms can be used in other commercial computer programs without an
agreement with the owners of the disassembled computer program.

The list of the available disassemblers includes tools for Windows like IDA Pro, PE Explorer, W32DASM, BORG Disassembler, HT Editor, diStorm64 and Linux like Bastard Disassembler, ciasdis, objdump, gdb, lida linux interactive disassembler, ldasm.

Code optimization is a stage during the compilation process. The stages of optimization are :

Intermediate representation optimization - data flow and code flow optimizations;

Code generation optimization - using the fast machine instructions,

During disassembly process, the control flow graph is built on sequences of 
instructions encoded in machine code. In [9], the control flow reconstruction is split in two parts:

Call graph - relationship between routines are highlighted; the routines are the nodes, and the calls and returns are the edges;

Control flow graph - jumps in the routine are highlighted, and it can be built for each routine; the nodes are called basic blocks, and the edges are jumps and fall-through edges; the basic blocks contain one-step executed instructions.

The disassembly process is presented in previous chapter together with its issues. There are two major classes of disassembly techniques [15].

Static disassembly – the binary file is not executed; the instruction stream is parsed as it is found in the machine code file to establish or approximate the computer program behavior;

Dynamic disassembly – the binary file is executed, and its execution is monitored to identify the instruction actions and behavior; the execution is made for some input sets, and as effect some instruction streams of the binary file can be avoided.


\section{Control Flow Graphs}

A control flow graph (CFG) is a directed, connected, graph that is used to represent all possible paths of execution that can occur within a program. Each vertex in the control flow graph represents a basic block, a linear sequence of instructions, and each edge between two vertices represents a possible control flow path [5]. A control flow graph has two artificial vertices, v ST ART and v EN D . The reason for these two artificial vertices is to ensure a connected graph where
every actual vertex without a predecessor is is connected with v ST ART and every actual vertex without a successor is connected with v ST ART . Because of this addition to the graph there is a path v ST ART ...v...v EN D for every vertex v within the graph.

For the purpose of explaining a control flow graph, instructions can be classi fied either as a Transfer Instruction (TI) or as a Non Transfer Instruction (NTI). Transfer instructions are the set of instructions that might transfer control flow to a part of the program different from the address of the next instruction. Unconditional jumps, where the control flow is transferred to the
target jump address; conditional jumps, where the control flow might be transferred to the target jump address; and subroutine calls, where the control flow is transferred to the invoked subroutine, are all examples of transfer instructions. Non transfer instructions are the set of instructions that will always transfer the control flow to the next instruction in sequence [6].

A basic block is a sequence of instructions that has a single point of entry and a single point of exit. A basic block either consists of zero or more non transfer instructions and ends with a single transfer instruction or consists of one or more non transfer instructions.

In addition to the classifications made by Cristina Cifuentes in [6], call basic block that has an indirect address reference (calculated during runtime), will be treated as if they had no outgoing edges. The reason for this addition to the classifications is to preserve context sensitivity when performing dynamic analysis. Context sensitivity in binary analysis is the concept that a function’s
behavior, and impact on the program, is relative to the calling context, or in other words, relative to the part of the code that called the function. Since functions are only control-dependent on a specific call-instruction as long as that instruction is part of the code that is currently being executed, adding edges to a function from each call would result in erroneous control-dependencies.

\section{Program Dependency Graphs}

The PDG makes explicit both the data and control dependences for each operation in a program. Data dependence graphs have provided some optimizing compilers with an explicit representation of the definition-use relationships implicitly present in a source program. A control flow graph has been the usual representation for the control flow relationships of a program; the control conditions on which an operation depends can be derived from such a graph.

Since both kinds of dependences are present in a single form, transformations like vectorization can treat control and data dependence uniformly. Program transformations such as code motion, which require interaction of the two types of dependences, can also be easily handled by our single graph.


\subsection{Control Dependency Graphs}
A control dependence graph (CDG) is a partially ordered, directed, acyclic graph where the vertices of the graph represents basic blocks and the edges between two vertices represents the control conditions on which the execution of the operations depends [10]. A control dependence between two vertices in a control flow graph exists if there is a conditional branch at one of the vertices
(the source of the dependence) that determines whether or not the other vertex (the sink of the dependence) is to be executed.

A vertex v q is control dependent on a vertex v p if (a) v q 6 = v p , (b) v q does not post-dominate v p and (c), there exists a path from v p to v q such that v q post-dominates every vertex on the path except for v p . A vertex v q can be control dependent on itself if there exists a path from v q to v q such that v q post-dominates every vertex on the path.

A static control dependence graph can contain fewer vertices than the initial control flow graph since there is no need to keep vertices in the static control dependence graph if they (a) depend on the same vertex as their immediate dominator and (b) doesn’t have any vertices depending on them. That being said, a static control dependence graph can still be larger than the initial control flow graph in terms of edges since a single vertex can be control dependent on hundreds or even thousands of vertices.

A dynamic control dependence graph, like its static counterpart, can contain fewer vertices than the initial control flow graph. Although, every vertex that controls whether or not another executed vertex should be executed, and every executed vertex that is control dependent on another vertex will be part of the dynamic control dependence graph. The maximum number of edges in a
dynamic control dependence graph is unbounded since almost every transition of control flow from one vertex to another will yield a new edge from the executed vertex to the vertex that it is control dependent upon.


\subsection{Data Dependency Graphs}
In this section we describe how DDGs are represented, starting with the logical graph representation. We then briefly outline our physical representation, and discuss how
graphs are traversed.

Data dependence analysis determines what the constraints are on how a piece of code can be reorganized.

Data dependences are constraints on the order in which statements may be execute.

Data dependencies may be represented using a directed acyclic graph (DAG).

Nodes are machine instructions. Edge i -> j if instruction j has a data dependence on instruction i.

Logical graph representation. Let I and J be two instructions in a program, and i, j be execution instances of I and J, respectively. i has a data dependency on j if i
uses data defined by j.

In the classical graph representation used in early works, such as, instruction instances are represented by vertices, and data or control dependencies are represented by outgoing
edges from vertices. We instead use a graph representation similar to the one proposed by Zhang
and Gupta in [8]. In their graphs, each static instruction is represented by a vertex, and data or control dependencies are represented by adding labeled edges between vertices.
The number of vertices is thus bounded by the size of the program, while the number of edges per vertex is determined by the length of execution. The label of an edge identifies the instruction instances involved in the corresponding dependency. For example, a data dependency of i on j is represented by adding an edge from I to J, labeled with instance(i) and instance(j)
i.e. the use-instance of I and the define-instance of J.


\section{Neural Network}
Neural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns. They interpret sensory data through a kind of machine perception, labeling or clustering raw input. The patterns they recognize are numerical, contained in vectors, into which all real-world data, be it images, sound, text or time series, must be translated.

Neural networks help us cluster and classify. You can think of them as a clustering and classification layer on top of the data you store and manage. They help to group unlabeled data according to similarities among the example inputs, and they classify data when they have a labeled dataset to train on.

Neural neworks are typically organized in layers. Layers are made up of a number of interconnected 'nodes' which contain an 'activation function'. Patterns are presented to the network via the 'input layer', which communicates to one or more 'hidden layers' where the actual processing is done via a system of weighted 'connections'. The hidden layers then link to an 'output layer' where the answer is output. Most ANNs contain some form of 'learning rule' which modifies the weights of the connections according to the input patterns that it is presented with. In a sense, ANNs learn by example as do their biological counterparts

he delta rule is often utilized by the most common class of ANNs called 'backpropagational neural networks' (BPNNs). Backpropagation is an abbreviation for the backwards propagation of error.

With the delta rule, as with other types of backpropagation, 'learning' is a supervised process that occurs with each cycle or 'epoch' (i.e. each time the network is presented with a new input pattern) through a forward activation flow of outputs, and the backwards error propagation of weight adjustments. More simply, when a neural network is initially presented with a pattern it makes a random 'guess' as to what it might be. It then sees how far its answer was from the actual one and makes an appropriate adjustment to its connection weights. Backpropagation performs a gradient descent within the solution's vector space towards a 'global minimum' along the steepest vector of the error surface. The global minimum is that theoretical solution with the lowest possible error. The error surface itself is a hyperparaboloid but is seldom 'smooth'.  Indeed, in most problems, the solution space is quite irregular with numerous 'pits' and 'hills' which may cause the network to settle down in a 'local minum' which is not the best overall solution. Since the nature of the error space can not be known a prioi, neural network analysis often requires a large number of individual runs to determine the best solution. Most learning rules have built-in mathematical terms to assist in this process which control the 'speed' (Beta-coefficient) and the 'momentum' of the learning. The speed of learning is actually the rate of convergence between the current solution and the global minimum. Momentum helps the network to overcome obstacles (local minima) in the error surface and settle down at or near the global miniumum.

Once a neural network is 'trained' to a satisfactory level it may be used as an analytical tool on other data. To do this, the user no longer specifies any training runs and instead allows the network to work in forward propagation mode only. New inputs are presented to the input pattern where they filter into and are processed by the middle layers as though training were taking place, however, at this point the output is retained and no backpropagation occurs. The output of a forward propagation run is the predicted model for the data which can then be used for further analysis and interpretation.

It is also possible to over-train a neural network, which means that the network has been trained exactly to respond to only one type of input; which is much like rote memorization. If this should happen then learning can no longer occur and the network is refered to as having been "grandmothered" in neural network jargon. In real-world applications this situation is not very useful since one would need a separate grandmothered network for each new kind of input.

However they work very well for:
capturing associations or discovering regularities within a set of patterns;
where the volume, number of variables or diversity of the data is very great;
the relationships between variables are vaguely understood; or,
the relationships are difficult to describe adequately with conventional approaches.


There are many advantages and limitations to neural network analysis and to discuss this subject properly we would have to look at each individual type of network, which isn't necessary for this general discussion.

Backpropagational neural networks (and many other types of networks) are in a sense the ultimate 'black boxes'. Apart from defining the general archetecture of a network and perhaps initially seeding it with a random numbers, the user has no other role than to feed it input and watch it train and await the output. In fact, it has been said that with backpropagation, "you almost don't know what you're doing". Some software freely available software packages (NevProp, bp, Mactivation) do allow the user to sample the networks 'progress' at regular time intervals, but the learning itself progresses on its own. The final product of this activity is a trained network that provides no equations or coefficients defining a relationship (as in regression) beyond it's own internal mathematics. The network 'IS' the final equation of the relationship.

Backpropagational networks also tend to be slower to train than other types of networks and sometimes require thousands of epochs. If run on a truly parallel computer system this issue is not really a problem, but if the BPNN is being simulated on a standard serial machine (i.e. a single SPARC, Mac or PC) training can take some time. This is because the machines CPU must compute the function of each node and connection separately, which can be problematic in very large networks with a large amount of data. However, the speed of most current machines is such that this is typically not much of an issue.

\subsection{Siamese Architecture}
Siamese networks are a special type of neural network architecture. Instead of a model learning to classify its inputs, the neural networks learns to differentiate between two inputs. 

A Siamese networks consists of two identical neural networks, each taking one of the two input images. The last layers of the two networks are then fed to a contrastive loss function , which calculates the similarity between the two images. There are two sister networks, which are identical neural networks, with the exact same weights. Each image in the image pair is fed to one of these networks. The networks are optimised using a contrastive loss function. The objective of the siamese architecture is not to classify input images, but to differentiate between them. So, a classification loss function (such as cross entropy) would not be the best fit. Instead, this architecture is better suited to use a contrastive function. Intuitively, this function just evaluates how well the network is distinguishing a given pair of images.

Similarity learning with deep CNNs is typically addressed using Siamese architectures.

For this domain, we
employ large siamese convolutional neural networks which a) are capable of learning generic image features useful for making predictions about unknown class distributions
even when very few examples from these new distributions are available; b) are easily trained using standard optimization techniques on pairs sampled from the source
data; and c) provide a competitive approach that does not rely upon domain-specific knowledge by instead exploiting deep learning techniques.

Siamese nets were first introduced in the early 1990s by Bromley and LeCun to solve signature verification as an image matching problem. A siamese neural network consists of twin networks which accept distinct inputs but are joined by an energy function at the top. This function computes some metric between the highestlevel feature representation on each side. The
parameters between the twin networks are tied. Weight tying guarantees that two extremely similar images could not possibly be mapped by their respective networks to very different locations in feature space because each network computes the same function. Also, the network is symmetric, so that whenever we present two distinct images to the twin networks, the top conjoining layer will compute the same metric as if we were to we present the same two images
but to the opposite twins.
